{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs  \n",
    "#from bs4 import BeautifulSoup as bs\n",
    "import urllib.request  \n",
    "import re\n",
    "import nltk\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# input WIKI URL...\n",
    "def input(url):\n",
    "    scraped_data = urllib.request.urlopen(url)\n",
    "    article = scraped_data.read()\n",
    "    parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "    #print(parsed_article)\n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "    #print(paragraphs)\n",
    "    article_text = \"\"\n",
    "    for p in paragraphs:  \n",
    "        article_text += p.text\n",
    "    #print(article_text)\n",
    "    return(article_text)\n",
    "\n",
    "# Sentence Tokenization\n",
    "def sent_tokens(text):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    #print(sents)\n",
    "    return(sents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(sentences):\n",
    "    sent_clean=[]\n",
    "    sent_clean1=[]\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    for sent in sentences:\n",
    "        sent_clean = \" \".join([word for word in nltk.word_tokenize(sent.lower()) if word not in stop_words])\n",
    "        #print(sent_clean)\n",
    "        sent_clean = [re.sub(r'[^a-zA-Z]',\" \",sent_clean)]\n",
    "        #print(sent_clean)\n",
    "        #sent_clean = [re.sub(r'\\s+', \" \",sent_clean(i))]\n",
    "        sent_clean1 +=sent_clean\n",
    "    #print(sent_clean1)\n",
    "    return(sent_clean1)\n",
    "\n",
    "# Extract word vectors\n",
    "def word_embedded(filename,cleaned_sentences,num_sentences): #file name with path\n",
    "    word_embeddings = {}\n",
    "    f = open(filename, encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = coefs\n",
    "    f.close()\n",
    "    sentence_vectors=[]\n",
    "    for i in cleaned_sentences:\n",
    "        if len(i)!=0:\n",
    "            v = sum([word_embeddings.get(w,np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
    "        else:\n",
    "            v= np.zeros((100))\n",
    "        sentence_vectors.append(v)\n",
    "        \n",
    "    #creating similarity_matrix\n",
    "    sim_mat = np.zeros([len(cleaned_sentences), len(cleaned_sentences)])\n",
    "    #print(sim_mat)\n",
    "    for i in range(len(cleaned_sentences)):\n",
    "        for j in range(len(cleaned_sentences)):\n",
    "            if i!=j:\n",
    "                sim_mat[i][j]=cosine_similarity(sentence_vectors[i].reshape(1,100),sentence_vectors[j].reshape(1,100))[0,0]\n",
    "    #Ranking the each Sentence\n",
    "    nx_graph = nx.from_numpy_matrix(sim_mat)\n",
    "    #nx_graph = nx.from_numpy_array(sim_mat)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    #print(scores)\n",
    "    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(cleaned_sentences)), reverse=True)\n",
    "    # Final output summary.....\n",
    "    summary=\"\"\n",
    "    for i in range(num_sentences):\n",
    "        summary_sents = ''.join(ranked_sentences[i][1])\n",
    "        summary +=(summary_sents+\".\")\n",
    "    return(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "solution fluid dynamics problem typically involves calculating various properties fluid velocity pressure density temperature functions space time .fluid mechanics also defined science deals study behaviour fluids either rest motion .fluid mechanics especially fluid dynamics active field research typically mathematically complex .particle image velocimetry experimental method visualizing analyzing fluid flow also takes advantage highly visual nature fluid flow .mechanical view fluid substance support shear stress fluid rest shape containing vessel .\n"
     ]
    }
   ],
   "source": [
    "article_text = input('https://en.wikipedia.org/wiki/Fluid_mechanics')\n",
    "#print(article_text)\n",
    "sentences= sent_tokens(article_text)\n",
    "#print(len(sentences))\n",
    "#print(sentences)\n",
    "cleaned_sentences = preprocess(sentences)\n",
    "print(len(cleaned_sentences))\n",
    "#summary=\"\"\n",
    "Summary = re.sub(r'\\s+', \" \",str(word_embedded('glove.6B.100d.txt',cleaned_sentences,5)))\n",
    "print(Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "https://www.quora.com/Natural-Language-Processing-What-are-algorithms-for-auto-summarize-text\n",
    "https://medium.com/@japneet121/word-vectorization-using-glove-76919685ee0b\n",
    "https://www.analyticsvidhya.com/blog/2018/11/introduction-text-summarization-textrank-python/\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
